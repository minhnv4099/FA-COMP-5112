#
#  Copyright (c) 2025
#  Minh NGUYEN <vnguyen9@lakeheadu.ca>
#
import logging

from langchain_core.language_models import BaseChatModel
from langchain_core.runnables.config import RunnableConfig
from typing_extensions import override

from .critic import CriticAgent
from ..base.agent import AgentAsNode, register
from ..base.utils import DirectionRouter
from ..utils import InputT, OutputT
from ..utils import load_image_content

logger = logging.getLogger(__name__)


@register(type="agent", name='verification')
class VerificationAgent(CriticAgent, AgentAsNode, name='Verification'):
    """
    The Verification Agent class
    """

    def __init__(
            self,
            metadata: dict = None,
            input_schema: InputT | dict = None,
            edges: dict[str, tuple[str]] = None,
            tool_schemas: list | list[dict] = None,
            output_schema: OutputT | list[OutputT] | list[dict] = None,
            model_name: str = None,
            model_provider: str = None,
            model_api_key: str = None,
            output_schema_as_tool: bool = None,
            chat_model: BaseChatModel = None,
            save_rendered_dir: str = None,
            anchor_script_path: str = None,
            verification_attempts: int = None,
            camera_setting_file: str = None,
            # templates
            template_file: str = None,
            **kwargs
    ):
        super().__init__(
            metadata=metadata,
            input_schema=input_schema,
            edges=edges,
            tool_schemas=tool_schemas,
            output_schema=output_schema,
            model_name=model_name,
            model_provider=model_provider,
            model_api_key=model_api_key,
            output_schema_as_tool=output_schema_as_tool,
            chat_model=chat_model,
            template_file=template_file,
            save_rendered_dir=save_rendered_dir,
            anchor_script_path=anchor_script_path,
            camera_setting_file=camera_setting_file,
            **kwargs
        )

        self.verification_attempts = verification_attempts
        self.verification_tries: int = 0

    @override
    def __call__(
            self,
            state: InputT | dict,
            runtime: RunnableConfig = None,
            context: RunnableConfig = None,
            config: RunnableConfig = None,
            **kwargs
    ):
        self._make_dirs()
        start_message = "-" * 50 + self.name + "-" * 50
        logger.info(start_message)

        # script after attempts fixing critics
        current_script = state['current_script']
        logger.info("Setup camera to capture fixes images")
        processed_script = self._process_script(current_script)

        # rendered images from critic on script generated by coding agent, without any fix
        rendered_images = state['rendered_images']
        # rendered images on script after attempts fixing critics
        fixed_rendered_images = self._run_to_get_rendered_images(processed_script)
        logger.info(f"Rendered images: {fixed_rendered_images}")

        critics_fixes = state['critics_fixes']
        critic_satisfied_solution = []
        # ri: rendered image
        # fi: fixed rendered image

        for i, (ri, fi) in enumerate(zip(rendered_images, fixed_rendered_images)):
            logger.info(f"image ({i + 1}/{len(fixed_rendered_images)}): {len(critics_fixes[i])} <critics, fixes>")
            # -----------------------------------------------------------------
            formatted_prompt = self.chat_template.invoke({
                'image': load_image_content(ri),
                "fixed_image": load_image_content(fi),
                "critics_fixes": critics_fixes[i],
            })
            response = self.chat_model_call(
                formatted_prompt,
                critics_fixes=critics_fixes[i]
            )
            # -----------------------------------------------------------------
            logger.info(f"<critics_satisfied_solution>: {response}")
            critic_satisfied_solution.extend(response)
        # ----------process critic_satisfied_solution list-----------
        #
        # ---------------------------------------------
        unsatisfied_critics = list(filter(
            lambda d: d['satisfied'].lower() != "yes",
            critic_satisfied_solution,
        ))

        logger.info(f"Unsatisfied critics: {len(unsatisfied_critics)}")

        fixes = [d['solution'] for d in unsatisfied_critics]
        logger.info(f"Solutions by verification: {fixes}")
        if fixes and self.verification_tries < self.verification_attempts:
            self.verification_tries += 1
            next_node = 'coding'
        else:
            next_node = 'user'
            self.verification_tries = 0

        end_message = "*" * (100 + len(self.name))
        logger.info(end_message)

        update_state = {
            'queries': fixes,
            'coding_task': 'improve',
            'is_sub_call': False,
            'caller': 'verification',
            'has_docs': False
        }

        return DirectionRouter.goto(state=update_state, node=next_node, method='command')

    @override
    def anchor_call(self, formatted_prompt: str, critics_fixes: list, *args, **kwargs):
        for i, d in enumerate(critics_fixes):
            if i % 2 == 0:
                d['satisfied'] = 'YES'
            else:
                d['satisfied'] = 'Partially, some details are not solved'

        return critics_fixes

    @override
    def chat_model_call(self, formatted_prompt: str, *args, **kwargs):
        ai_message = self.chat_model.invoke(formatted_prompt)
        return ai_message.tool_calls[-1]['args']['css_list']
