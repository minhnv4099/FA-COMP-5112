#
#  Copyright (c) 2025
#  Minh NGUYEN <vnguyen9@lakeheadu.ca>
#
from typing import Optional

from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langgraph.config import RunnableConfig
from langgraph.runtime import Runtime
from langgraph.types import Command, Send
from typing_extensions import override, Any

from src.base.agent import AgentAsNode, InputT, register
from src.base.tool import execute_script


@register(type="agent", name='coding')
class CodingAgent(AgentAsNode, name='Coding', use_model=True):
    """
    The Coding Agent class
    """

    @override
    def __init__(
            self,
            metadata: dict = None,
            input_schema: InputT = None,
            edges: dict[str, tuple[str]] = None,
            tool_schemas: list = None,
            output_schema: Any = None,
            model_name: str = None,
            model_provider: str = None,
            model_api_key: str = None,
            output_schema_as_tool: bool = None,
            chat_model: BaseChatModel = None,
            anchor_folder: str = None,
            system_prompt: str = None,
            human_prompt: str = None,
            **kwargs
    ):
        super().__init__(
            metadata,
            input_schema,
            edges,
            tool_schemas,
            output_schema,
            model_name,
            model_provider,
            model_api_key,
            output_schema_as_tool,
            chat_model,
            **kwargs
        )

        self.anchor_folder = anchor_folder
        """Anchor folder containing scripts generated by model each attempt"""

        self.system_prompt = SystemMessagePromptTemplate.from_template("""
            You're smart assistance and able to generate Python code in Blender
            Your mission are:
                1. Base on given scripts (if available), "query/subtask " and given "docs" to generate code.

            The format looks like:
            ```JSON block
                subtask: <subtask> 
                docs: [<docs>]
                previous: [<previous_code>]
            """)

        self.human_generate_code_prompt = HumanMessagePromptTemplate.from_template("""
        Help me to write python code using below information:
            subtask: {subtask}
            docs: {docs} 
            previous: {previous_code}
            
        With following instructions:
            - Not include 'bmesh' module
            - Delete all materials before writing by import 2 lines after import libraries
                ```
                    bpy.ops.object.select_all(action='SELECT')
                    bpy.ops.object.delete(use_global=False)
                ```
        """)

        self.human_apply_improvements_prompt = HumanMessagePromptTemplate.from_template("""""")
        self.human_fix_error_prompt = HumanMessagePromptTemplate.from_template("""""")

        self.subtask_offset = 0

    @override
    def __call__(
            self,
            state: InputT | dict,
            runtime: Optional[Runtime] = None,
            context: Optional[Runtime] = None,
            config: Optional[Runtime[RunnableConfig]] = None,
            **kwargs
    ):
        # --------------- model works ---------------
        context = runtime.context
        print("coding", state, sep='\n\t')
        if state['coding_task'] == 'generate':
            script = self._generate_script(state, context)
        elif state['coding_task'] == "improve":
            script = self._apply_improvements(state, context)
        else:
            script = self._fix_error(state, context=context)
        # -------------------------------------------
        if isinstance(script, (Send, Command)):
            return script

        return {"script": script}

    def _prepare_by_adding_human_prompt(self, human_prompt):
        return ChatPromptTemplate([
            self.system_prompt,
            human_prompt,
        ])

    def _fix_error(self, state, context) -> str:
        chat_prompt = self._prepare_by_adding_human_prompt(self.human_fix_error_prompt)
        print(state)
        exit()
        formatted_prompt = chat_prompt.invoke({...})
        # ---------------------------------------------------
        improved_script = self._write_code(formatted_prompt, context)
        # ---------------------------------------------------

        return improved_script

    def _apply_improvements(self, state, context):
        chat_prompt = ChatPromptTemplate([
            self.system_prompt,
            self.human_apply_improvements_prompt,
        ])

        formatted_prompt = chat_prompt.invoke({...})
        # ---------------------------------------------------
        improved_script = self._write_code(formatted_prompt, context)
        # ---------------------------------------------------
        return improved_script

    def _generate_script(self, state, context):
        chat_prompt = ChatPromptTemplate([
            self.system_prompt,
            self.human_generate_code_prompt,
        ])
        subscripts = []
        for i, (query, docs) in enumerate(state['retrieved_docs']):
            formatted_prompt = chat_prompt.invoke({
                "subtask": query,
                "docs": docs,
                "previous_code": subscripts,
            })
            # ---------------------------------------------------
            message = self._write_code(formatted_prompt, context)
            # ---------------------------------------------------
            return message
        return subscripts[-1]

    def _write_code(self, formatted_prompt, context):
        while True:
            # response = self.chat_model.invoke(formatted_prompt)
            # print(response.tool_calls[-1]['args'])
            # try:
            #     generated_script = response.tool_calls[-1]['args']['script']
            # except KeyError as e:
            #     exit()
            #
            # anchor_file = os.path.join(self.anchor_folder, f"anchor.py")
            # write_script.invoke({
            #     "script": generated_script,
            #     "file_path": anchor_file
            # })

            result = execute_script.invoke(input={"script": "assets/blender_script/anchor_2.py"})
            error = result['error']
            if len(error) == 0:
                return None
            else:
                # context['coding_task'] = 'fix'
                return Command(
                    goto='retriever',
                    update={
                        'coding_task': 'fix',
                        'queries': [error, ]
                    },
                )
